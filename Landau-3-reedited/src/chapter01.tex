\chapter{THE BASIC CONCEPTS OF QUANTUM MECHANICS}
\section{The uncertainty principle}\label{The uncertainty principle}
When we attempt to apply classical mechanics and electrodynamics to explain atomic phenomena, they lead to results which are in obvious conflict with experiment. This is very clearly seen from the contradiction obtained on applying ordinary electrodynamics to a model of an atom in which the electrons move round the nucleus in classical orbits. During such motion, as in any accelerated motion of charges, the electrons would have to emit electromagnetic waves continually. By this emission, the electrons would lose their energy, and this would eventually cause them to fall into the nucleus. Thus, according to classical electrodynamics, the atom would be unstable, which does not at all agree with reality.


This marked contradiction between theory and experiment indicates that the construction of a theory applicable to atomic phenomena—that is, phenomena occurring in particles of very small mass at very small distances—demands a fundamental modification of the basic physical concepts and laws.


As a starting-point for an investigation of these modifications, it is convenient to take the experimentally observed phenomenon known as \textit{electron diffraction}.\footnote{The phenomenon of electron diffraction was in fact discovered after quantum mechanics was invented. In our discussion, however, we shall not adhere to the historical sequence of development of the theory, but shall endeavour to construct it in such a way that the connection between the basic principles of quantum mechanics and the experimentally observed phenomena is most clearly shown.
} It is found that, when a homogeneous beam of electrons passes through a crystal, the emergent beam exhibits a pattern of alternate maxima and minima of intensity, wholly similar to the diffraction pattern observed in the diffraction of electromagnetic waves. Thus, under certain conditions, the behaviour of material particles—in this case, the electrons—displays features belonging to wave processes.


How markedly this phenomenon contradicts the usual ideas of motion is best seen from the following imaginary experiment, an idealization of the experiment of electron diffraction by a crystal. Let us imagine a screen impermeable to electrons, in which two slits are cut. On observing the passage of a beam of electrons\footnote{The beam is supposed so rarefied that the interaction of the particles in it plays no part.
} through one of the slits, the other being covered, we obtain, on a continuous screen placed behind the slit, some pattern of intensity distribution; in the same way, by uncovering the second slit and covering the first, we obtain another pattern. On observing the passage of the beam through both slits, we should expect, on the basis of ordinary classical ideas, a pattern which is a simple superposition of the other two: each electron, moving in its path, passes through one of the slits and has no effect on the electrons passing through the other slit. The phenomenon of electron diffraction shows, however, that in reality we obtain a diffraction pattern which, owing to interference, does not at all correspond to the sum of the patterns given by each slit separately. It is clear that this result can in no way be reconciled with the idea that electrons move in paths.


Thus the mechanics which governs atomic phenomena—\textit{quantum mechanics} or \textit{wave mechanics}—must be based on ideas of motion which are fundamentally different from those of classical mechanics. In quantum mechanics there is no such concept as the path of a particle. This forms the content of what is called the uncertainty principle, one of the fundamental principles of quantum mechanics, discovered by W. Heisenberg in 1927.\footnote{It is of interest to note that the complete mathematical formalism of quantum mechanics was constructed by W. Heisenberg and E. Schrödinger in 1925–6, before the discovery of the uncertainty principle, which revealed the physical content of this formalism.
}


In that it rejects the ordinary ideas of classical mechanics, the uncertainty principle might be said to be negative in content. Of course, this principle in itself does not suffice as a basis on which to construct a new mechanics of particles. Such a theory must naturally be founded on some positive assertions, which we shall discuss below (§\ref{The principle of superposition}). However, in order to formulate these assertions, we must first ascertain the statement of the problems which confront quantum mechanics. To do so, we first examine the special nature of the interrelation between quantum mechanics and classical mechanics. A more general theory can usually be formulated in a logically complete manner, independently of a less general theory which forms a limiting case of it. Thus, relativistic mechanics can be constructed on the basis of its own fundamental principles, without any reference to Newtonian mechanics. It is in principle impossible, however, to formulate the basic concepts of quantum mechanics without using classical mechanics. The fact that an electron\footnote{In this and the following sections we shall, for brevity, speak of “an electron”, meaning in general any object of a quantum nature, i.e. a particle or system of particles obeying quantum mechanics and not classical mechanics.
} has no definite path means that it has also, in itself, no other dynamical characteristics\footnote{We refer to quantities which characterize the motion of the electron, and not to those, such as the charge and the mass, which relate to it as a particle; these are parameters.
}. Hence it is clear that, for a system composed only of quantum objects, it would be entirely impossible to construct any logically independent mechanics. The possibility of a quantitative description of the motion of an electron requires the presence also of physical objects which obey classical mechanics to a sufficient degree of accuracy. If an electron interacts with such a “classical object”, the state of the latter is, generally speaking, altered. The nature and magnitude of this change depend on the state of the electron, and therefore may serve to characterize it quantitatively.


In this connection the “classical object” is usually called \textit{apparatus}, and its interaction with the electron is spoken of as \textit{measurement}. However, it must be emphasized that we are here not discussing a process of measurement in which the physicist-observer takes part. By \textit{measurement}, in quantum mechanics, we understand any process of interaction between classical and quantum objects, occurring apart from and independently of any observer. The importance of the concept of measurement in quantum mechanics was elucidated by N. Bohr.


We have defined “apparatus” as a physical object which is governed, with sufficient accuracy, by classical mechanics. Such, for instance, is a body of large enough mass. However, it must not be supposed that apparatus is necessarily macroscopic. Under certain conditions, the part of apparatus may also be taken by an object which is microscopic, since the idea of “with sufficient accuracy” depends on the actual problem proposed. Thus, the motion of an electron in a Wilson chamber is observed by means of the cloudy track which it leaves, and the thickness of this is large compared with atomic dimensions; when the path is determined with such low accuracy, the electron is an entirely classical object.


Thus quantum mechanics occupies a very unusual place among physical theories: it contains classical mechanics as a limiting case, yet at the same time it requires this limiting case for its own formulation.


We may now formulate the problem of quantum mechanics. A typical problem consists in predicting the result of a subsequent measurement from the known results of previous measurements. Moreover, we shall see later that, in comparison with classical mechanics, quantum mechanics, generally speaking, restricts the range of values which can be taken by various physical quantities (for example, energy): that is, the values which can be obtained as a result of measuring the quantity concerned. The methods of quantum mechanics must enable us to determine these admissible values.


The measuring process has in quantum mechanics a very important property: it always affects the electron subjected to it, and it is in principle impossible to make its effect arbitrarily small, for a given accuracy of measurement. The more exact the measurement, the stronger the effect exerted by it, and only in measurements of very low accuracy can the effect on the measured object be small. This property of measurements is logically related to the fact that the dynamical characteristics of the electron appear only as a result of the measurement itself. It is clear that, if the effect of the measuring process on the object of it could be made arbitrarily small, this would mean that the measured quantity has in itself a definite value independent of the measurement.


Among the various kinds of measurement, the measurement of the coordinates of the electron plays a fundamental part. Within the limits of applicability of quantum mechanics, a measurement of the coordinates of an electron can always be performed\footnote{Once again we emphasize that, in speaking of “performing a measurement”, we refer to the interaction of an electron with a classical “apparatus”, which in no way presupposes the presence of an external observer.
} with any desired accuracy.


Let us suppose that, at definite time intervals $ \Delta t $, successive measurements of the coordinates of an electron are made. The results will not in general lie on a smooth curve. On the contrary, the more accurately the measurements are made, the more discontinuous and disorderly will be the variation of their results, in accordance with the non-existence of a path of the electron. A fairly smooth path is obtained only if the coordinates of the electron are measured with a low degree of accuracy, as for instance from the condensation of vapour droplets in a Wilson chamber.


If now, leaving the accuracy of the measurements unchanged, we diminish the intervals $ \Delta t $ between measurements, then adjacent measurements, of course, give neighbouring values of the coordinates. However, the results of a series of successive measurements, though they lie in a small region of space, will be distributed in this region in a wholly irregular manner, lying on no smooth curve. In particular, as $ \Delta t $ tends to zero, the results of adjacent measurements by no means tend to lie on one straight line.


This circumstance shows that, in quantum mechanics, there is no such concept as the velocity of a particle in the classical sense of the word, i.e. the limit to which the difference of the coordinates at two instants, divided by the interval $ \Delta t $ between these instants, tends as $ \Delta t $ tends to zero. However, we shall see later that in quantum mechanics, nevertheless, a reasonable definition of the velocity of a particle at a given instant can be constructed, and this velocity passes into the classical velocity as we pass to classical mechanics. But whereas in classical mechanics a particle has definite coordinates and velocity at any given instant, in quantum mechanics the situation is entirely different. If, as a result of measurement, the electron is found to have definite coordinates, then it has no definite velocity whatever. Conversely, if the electron has a definite velocity, it cannot have a definite position in space. For the simultaneous existence of the coordinates and velocity would mean the existence of a definite path, which the electron has not. Thus, in quantum mechanics, the coordinates and velocity of an electron are quantities which cannot be simultaneously measured exactly, i.e. they cannot simultaneously have definite values. We may say that the coordinates and velocity of the electron are quantities which do not exist simultaneously. In what follows we shall derive the quantitative relation which determines the possibility of an inexact measurement of the coordinates and velocity at the same instant.


A complete description of the state of a physical system in classical mechanics is effected by stating all its coordinates and velocities at a given instant; with these initial data, the equations of motion completely determine the behaviour of the system at all subsequent instants. In quantum mechanics such a description is in principle impossible, since the coordinates and the corresponding velocities cannot exist simultaneously. Thus a description of the state of a quantum system is effected by means of a smaller number of quantities than in classical mechanics, i.e. it is less detailed than a classical description.


A very important consequence follows from this regarding the nature of the predictions made in quantum mechanics. Whereas a classical description suffices to predict the future motion of a mechanical system with complete accuracy, the less detailed description given in quantum mechanics evidently cannot be enough to do this. This means that, even if an electron is in a state described in the most complete manner possible in quantum mechanics, its behaviour at subsequent instants is still in principle uncertain. Hence quantum mechanics cannot make completely definite predictions concerning the future behaviour of the electron. For a given initial state of the electron, a subsequent measurement can give various results. The problem in quantum mechanics consists in determining the probability of obtaining various results on performing this measurement. It is understood, of course, that in some cases the probability of a given result of measurement may be equal to unity, i.e. certainty, so that the result of that measurement is unique.


All measuring processes in quantum mechanics may be divided into two classes. In one, which contains the majority of measurements, we find those which do not, in any state of the system, lead with certainty to a unique result. The other class contains measurements such that for every possible result of measurement there is a state in which the measurement leads with certainty to that result. These latter measurements, which may be called \textit{predictable}, play an important part in quantum mechanics. The quantitative characteristics of a state which are determined by such measurements are what are called \textit{physical quantities} in quantum mechanics. If in some state a measurement gives with certainty a unique result, we shall say that in this state the corresponding physical quantity has a definite value. In future we shall always understand the expression “physical quantity” in the sense given here.


We shall often find in what follows that by no means every set of physical quantities in quantum mechanics can be measured simultaneously, i.e. can all have definite values at the same time. We have already mentioned one example, namely the velocity and coordinates of an electron. An important part is played in quantum mechanics by sets of physical quantities having the following property: these quantities can be measured simultaneously, but if they simultaneously have definite values, no other physical quantity (not being a function of these) can have a definite value in that state. We shall speak of such sets of physical quantities as \textit{complete sets}.


Any description of the state of an electron arises as a result of some measurement. We shall now formulate the meaning of a \textit{complete description} of a state in quantum mechanics. Completely described states occur as a result of the simultaneous measurement of a complete set of physical quantities. From the results of such a measurement we can, in particular, determine the probability of various results of any subsequent measurement, regardless of the history of the electron prior to the first measurement.


From now on (except in §14) we shall understand by the states of a quantum system just these completely described states.
\section{The principle of superposition}\label{The principle of superposition}
The radical change in the physical concepts of motion in quantum mechanics as compared with classical mechanics demands, of course, an equally radical change in the mathematical formalism of the theory. We must therefore consider first of all the way in which states are described in quantum mechanics.


We shall denote by $ q $ the set of coordinates of a quantum system, and by $ \mathrm{d}q $ the product of the differentials of these coordinates. This $ \mathrm{d}q $ is called an element of volume in the \textit{configuration space} of the system; for one particle, $ \mathrm{d}q $ coincides with an element of volume $ \mathrm{d}V $ in ordinary space.


The basis of the mathematical formalism of quantum mechanics lies in the proposition that the state of a system can be described by a definite (in general complex) function $ \Psi(q) $ of the coordinates. The square of the modulus of this function determines the probability distribution of the values of the coordinates: $ |\Psi|^2\mathrm{d}q $ is the probability that a measurement performed on the system will find the values of the coordinates to be in the element $ \mathrm{d}q $ of configuration space. The function $ \Psi $ is called the \textit{wave function} of the system.\footnote{It was first introduced into quantum mechanics by Schrödinger in 1926.}


A knowledge of the wave function allows us, in principle, to calculate the probability of the various results of any measurement (not necessarily of the coordinates) also. All these probabilities are determined by expressions bilinear in $ \Psi $ and $ \Psi^* $. The most general form of such an expression is
\begin{equation}\label{2.1}
\iint\Psi(q)\Psi^*(q')\phi(q,q')\mathrm{d}q\mathrm{d}q',
\end{equation}
where the function $\phi(q, q′)$ depends on the nature and the result of the measurement, and the integration is extended over all configuration space. The probability $ \Psi\Psi^* $ of various values of the coordinates is itself an expression of this type.\footnote{It is obtained from \eqref{2.1} when $\phi(q, q′) = \delta(q-q_0)\delta(q′-q_0)$, where $ \delta $ denotes the delta function, defined in §5 below; $ q_0 $ denotes the value of the coordinates whose probability is required.
}

The state of the system, and with it the wave function, in general varies with time. In this sense the wave function can be regarded as a function of time also. If the wave function is known at some initial instant, then, from the very meaning of the concept of complete description of a state, it is in principle determined at every succeeding instant. The actual dependence of the wave function on time is determined by equations which will be derived later.

The sum of the probabilities of all possible values of the coordinates of the system must, by definition, be equal to unity. It is therefore necessary that the result of integrating $ |\Psi|^2 $ over all configuration space should be equal to unity:
\begin{equation}\label{2.2}
\int|\Psi|^2\mathrm{d}q=1.
\end{equation}
This equation is what is called the \textit{normalization condition} for wave functions. If the integral of $ |\Psi|^2 $ converges, then by choosing an appropriate constant coefficient the function $ \Psi $ can always be, as we say, \textit{normalized}. However, we shall see later that the integral of $ |\Psi|^2 $ may diverge, and then $ \Psi $ cannot be normalized by the condition \eqref{2.2}. In such cases $ |\Psi|^2 $ does not, of course, determine the absolute values of the probability of the coordinates, but the ratio of the values of $ |\Psi|^2 $ at two different points of configuration space determines the relative probability of the corresponding values of the coordinates.

Since all quantities calculated by means of the wave function, and having a direct physical meaning, are of the form \eqref{2.1}, in which $ \Psi $ appears multiplied by $ \Psi^* $, it is clear that the normalized wave function is determined only to within a constant \textit{phase factor} of the form $ \mathrm{e}^{\mathrm{i}\alpha} $ (where $ \alpha $ is any real number). This indeterminacy is in principle irremovable; it is, however, unimportant, since it has no effect upon any physical results.

The positive content of quantum mechanics is founded on a series of propositions concerning the properties of the wave function. These are as follows.

Suppose that, in a state with wave function $ \Psi_1(q) $, some measurement leads with certainty to a definite result (result 1), while in a state with $ \Psi_2(q) $ it leads to result 2. Then it is assumed that every linear combination of $ \Psi_1 $ and $ \Psi_2$, i.e. every function of the form $ \mathrm{c}_1\Psi_1(q)+\mathrm{c}_2\Psi_2(q) $ (where $ \mathrm{c}_1 $ and $ \mathrm{c}_1 $ are constants), gives a state in which that measurement leads to either result 1 or result 2. Moreover, we can assert that, if we know the time dependence of the states, which for the one case is given by the function $ \Psi_1(q,t) $, and for the other by $ \Psi_2(q,t) $, then any linear combination also gives a possible dependence of a state on time. These propositions constitute what is called the \textit{principle of superposition of states}, the chief positive principle of quantum mechanics. In particular, it follows from this principle that all equations satisfied by wave functions must be linear in $ \Psi $.

Let us consider a system composed of two parts, and suppose that the state of this system is given in such a way that each of its parts is completely described.\footnote{This, of course, means that the state of the whole system is completely described also. However, we emphasize that the converse statement is by no means true: a complete description of the state of the whole system does not in general completely determine the states of its individual parts (see also §14).
} Then we can say that the probabilities of the coordinates $ q_1 $ of the first part are independent of the probabilities of the coordinates $ q_2 $ of the second part, and therefore the probability distribution for the whole system should be equal to the product of the probabilities of its parts. This means that the wave function $ \Psi_{12}(q_1,q_2) $ of the system can be represented in the form of a product of the wave functions $ \Psi_1(q_1) $ and $ \Psi_2(q_2) $ of its parts:
\begin{equation}\label{2.3}
\Psi_{12}(q_1,q_2)=\Psi_1(q_1)\Psi_2(q_2).
\end{equation}
If the two parts do not interact, then this relation between the wave function of the system and those of its parts will be maintained at future instants also, i.e. we can write
\begin{equation}\label{2.4}
\Psi_{12}(q_1,q_2,t)=\Psi_1(q_1,t)\Psi_2(q_2,t).
\end{equation}
\section{Operators}\label{Operators}
Let us consider some physical quantity $ f $ which characterizes the state of a quantum system. Strictly, we should speak in the following discussion not of one quantity, but of a complete set of them at the same time. However, the discussion is not essentially changed by this, and for brevity and simplicity we shall work below in terms of only one physical quantity.

The values which a given physical quantity can take are called in quantum mechanics its \textit{eigenvalues}, and the set of these is referred to as the \textit{spectrum of eigenvalues} of the given quantity. In classical mechanics, generally speaking, quantities run through a continuous series of values. In quantum mechanics also there are physical quantities (for instance, the coordinates) whose eigenvalues occupy a continuous range; in such cases we speak of a \textit{continuous spectrum of eigenvalues}. As well as such quantities, however, there exist in quantum mechanics others whose eigenvalues form some discrete set; in such cases we speak of a \textit{discrete spectrum}.

We shall suppose for simplicity that the quantity $ f $ considered here has a discrete spectrum; the case of a continuous spectrum will be discussed in §5. The eigenvalues of the quantity $ f $ are denoted by $ f_n $, where the suffix $ n $ takes the values $ 0,1,2,3,\dots. $. We also denote the wave function of the system, in the state where the quantity $ f $ has the value $ f_n $, by $ \Psi_n $. The wave functions $ \Psi_n $ are called the \textit{eigenfunctions} of the given physical quantity $ f $. Each of these functions is supposed normalized, so that
\begin{equation}\label{3.1}
\int|\Psi_n|^2\mathrm{d}q=1.
\end{equation}
If the system is in some arbitrary state with wave function $ \Psi $, a measurement of the quantity $ f $ carried out on it will give as a result one of the eigenvalues $ f_n $. In accordance with the principle of superposition, we can assert that the wave function $ \Psi $ must be a linear combination of those eigenfunctions $ \Psi_n $ which correspond to the values $ f_n $ that can be obtained, with probability different from zero, when a measurement is made on the system and it is in the state considered. Hence, in the general case of an arbitrary state, the function $ \Psi $ can be represented in the form of a series
\begin{equation}\label{3.2}
\Psi=\sum a_n\Psi_n,
\end{equation}
where the summation extends over all $ n $, and the $ a_n $ are some constant coefficients.

Thus we reach the conclusion that any wave function can be, as we say, expanded in terms of the eigenfunctions of any physical quantity. A set of functions in terms of which such an expansion can be made is called a \textit{complete} (or \textit{closed}) set.

The expansion \eqref{3.2} makes it possible to determine the probability of finding (i.e. the probability of getting the corresponding result on measurement), in a system in a state with wave function $ \Psi $, any given value $ f_n $ of the quantity $ f $. For, according to what was said in the previous section, these probabilities must be determined by some expressions bilinear in $ \Psi $ and $ \Psi^* $, and therefore must be bilinear in $ a_n $ and $ a_n^* $. Furthermore, these expressions must, of course, be positive. Finally, the probability of the value $ f_n $ must become unity if the system is in a state with wave function $ \Psi=\Psi_n $, and must become zero if there is no term containing $ \Psi_n $ in the expansion \eqref{3.2} of the wave function $ \Psi $. The only essentially positive quantity satisfying these conditions is the square of the modulus of the coefficient $ a_n $. Thus we reach the result that the squared modulus $ |a_n|^2 $ of each coefficient in the expansion \eqref{3.2} determines the probability of the corresponding value $ f_n $ of the quantity $ f $ in the state with wave function $ \Psi $. The sum of the probabilities of all possible values $ f_n $ must be equal to unity; in other words, the relation
\begin{equation}\label{3.3}
\sum_{n}|a_n|^2=1
\end{equation}
must hold.

If the function $ \Psi $ were not normalized, then the relation \eqref{3.3} would not hold either. The sum $ \sum|a_n|^2 $ would then be given by some expression bilinear in $ \Psi $ and $ \Psi^* $, and becoming unity when $ \Psi $ was normalized. Only the integral $ \int\Psi\Psi^*\mathrm{d}q $ is such an expression. Thus the equation
\begin{equation}\label{3.4}
\sum_{n}a_na_n^*=\int\Psi\Psi^*\mathrm{d}q
\end{equation}
must hold.

On the other hand, multiplying by $ \Psi $ the expansion of the function $ \Psi^* $ (the complex conjugate of $ \Psi $), and integrating, we obtain
\[ \int\Psi\Psi^*\mathrm{d}q=\sum_{n}a_n^*\int\Psi_n^*\Psi\mathrm{d}q. \]
Comparing this with \eqref{3.4}, we have
\[ \sum_{n}a_na_n^*=\sum_{n}a_n^*\int\Psi_n^*\Psi\mathrm{d}q, \]
from which we derive the following formula determining the coefficients $ a_n $ in the expansion of the function $ \Psi $ in terms of the eigenfunctions $ \Psi_n $:
\begin{equation}\label{3.5}
a_n=\int\Psi\Psi_n^*\mathrm{d}q.
\end{equation}
If we substitute here from \eqref{3.2}, we obtain
\[ a_n=\sum_{m}a_m\int\Psi_m\Psi_n^*\mathrm{d}q \]
from which it is evident that the eigenfunctions must satisfy the conditions
\begin{equation}\label{3.6}
\int\Psi_m\Psi_n^*\mathrm{d}q=\delta_{nm}
\end{equation}
where $ \delta_{nm} = 1  $ for $  n = m $ and $ \delta_{nm} = 0  $ for $ n \ne m $. The fact that the integrals of the products $ \Psi_m\Psi_n^* $ with $ n \ne m $ vanish is called the \textit{orthogonality} of the functions $ \Psi_n $. Thus the set of eigenfunctions $ \Psi_n $ forms a complete set of normalized and orthogonal (or, for brevity, \textit{orthonormal}) functions.

We shall now introduce the concept of the mean value $ \bar{f} $ of the quantity $ f $ in the given state. In accordance with the usual definition of mean values, we define $ \bar{f} $ as the sum of all the eigenvalues $ f_n $ of the given quantity, each multiplied by the corresponding probability $ |a_n|^2 $. Thus
\begin{equation}\label{3.7}
\bar{f}=\sum_{n}f_n|a_n|^2.
\end{equation}
We shall write $ \bar{f} $ in the form of an expression which does not contain the coefficients $ a_n $ in the expansion of the function $ \Psi $, but this function itself. Since the products $ a_na_n* $ appear in \eqref{3.7}, it is clear that the required expression must be bilinear in $ \Psi $ and $ \Psi^* $. We introduce a mathematical operator, which we denote\footnote{By convention, we shall always denote operators by letters with circumflexes.
} by $ \hat{f} $ and define as follows. Let $ (\hat{f}\Psi) $ denote the result of the operator $ \hat{f} $ acting on the function $ \Psi $. We define $ \hat{f} $ in such a way that the integral of the product of $ (\hat{f}\Psi) $ and the complex conjugate function $ \Psi^* $ is equal to the mean value $ \bar{f} $:
\begin{equation}\label{3.8}
\bar{f}=\int\Psi^*(\hat{f}\Psi)\mathrm{d}q.
\end{equation}
It is easily seen that, in the general case, the operator $ \hat{f} $ is a linear\footnote{An operator is said to be linear if it has the properties
\[ \hat{f}(\Psi_1+\Psi_2)=\hat{f}(\Psi_1)+\hat{f}(\Psi_2)\text{ and }\hat{f}(a\Psi)=a\hat{f}\Psi \]
where $ \Psi_1 $ and $ \Psi_2 $ are arbitrary functions and $ a $ is an arbitrary constant.} integral operator. For, using the expression \eqref{3.5} for $ a_n $, we can rewrite the definition \eqref{3.7} of the mean value in the form
\[ \bar{f}=\sum_{n}f_na_na_n^*=\int\Psi^*(\sum_{n}a_nf_n\Psi_n)\mathrm{d}q. \]
Comparing this with \eqref{3.8}, we see that the result of the operator $ \hat{f} $ acting on the function $ \Psi $ has the form
\begin{equation}\label{3.9}
(\hat{f}\Psi)=\sum_{n}a_nf_n\Psi_n.
\end{equation}
If we substitute here the expression \eqref{3.5} for $ a_n $, we find that $ \hat{f} $ is an integral operator of the form
\begin{equation}\label{3.10}
(\hat{f}\Psi)=\int K(q,q')\Psi(q')\mathrm{d}q',
\end{equation}
where the function $ K(q, q') $ (called the \textit{kernel} of the operator) is
\begin{equation}\label{3.11}
K(q,q')=\sum_{n}f_n\Psi_n^*(q')\psi_n(q).
\end{equation}
Thus, for every physical quantity in quantum mechanics, there is a definite corresponding linear operator.

It is seen from \eqref{3.9} that, if the function $ \Psi $ is one of the eigenfunctions $ \Psi_n $(so that all the an except one are zero), then, when the operator $ \hat{f} $ acts on it, this function is simply multiplied by the corresponding eigenvalue $ f_n $:
\begin{equation}\label{3.12}
\hat{f}\Psi_n=f_n\Psi_n.
\end{equation}
(In what follows we shall always omit the parentheses in the expression $ (\hat{f\Psi}) $, where this cannot cause any misunderstanding; the operator is taken to act on the expression which follows it.) Thus we can say that the eigenfunctions of the given physical quantity $ f $ are the solutions of the equation
\[ \hat{f}\Psi=f\Psi, \]
where $ f $ is a constant, and the eigenvalues are the values of this constant for which the above equation has solutions satisfying the required conditions. As we shall see below, the form of the operators for various physical quantities can be determined from direct physical considerations, and then the above property of the operators enables us to find the eigenfunctions and eigenvalues by solving the equations $ \hat{f}\Psi=f\Psi $.

Both the eigenvalues of a real physical quantity and its mean value in every state are real. This imposes a restriction on the corresponding operators. Equating the expression \eqref{3.8} to its complex conjugate, we obtain the relation
\begin{equation}\label{3.13}
\int\Psi^*(\hat{f}\Psi)\mathrm{d}q=\int\psi(\hat{f}^*\Psi^*)\mathrm{d}q,
\end{equation}
where $ \hat{f}^* $ denotes the operator which is the complex conjugate of $ \hat{f} $.\footnote{By definition, if for the operator $ \hat{f} $ we have $ \hat{f}\Psi=\phi $, then the complex conjugate operator $ \hat{f}^* $ is that for which we have $ \hat{f}^*\Psi^*=\phi^* $.
} This relation does not hold in general for an arbitrary linear operator, so that it is a restriction on the form of the operator $ \hat{f} $. For an arbitrary operator $ \hat{f} $ we can find what is called the \textit{transposed operator} $ \tilde{\hat{f}} $, defined in such a way that
\begin{equation}\label{3.14}
\int\Phi(\hat{f}^*\Psi)\mathrm{d}q=\int\Psi(\tilde{\hat{f}}\Phi)\mathrm{d}q
\end{equation}
where $ \Psi $ and $ \Phi $ are two different functions. If we take, as the function $ \Phi $, the function $ \Psi^* $ which is the complex conjugate of $ \Psi $, then a comparison with \eqref{3.13} shows that we must have
\begin{equation}\label{3.15}
\tilde{\hat{f}}=\hat{f}^*
\end{equation}
Operators satisfying this condition are said to be \textit{Hermitian}.\footnote{For a linear integral operator of the form \eqref{3.10}, the Hermitian condition means that the kernel of the operator must be such that $ K (q, q′) = K^*(q′, q) $.
} Thus the operators corresponding, in the mathematical formalism of quantum mechanics, to real physical quantities must be Hermitian.

We can formally consider complex physical quantities also, i.e. those whose eigenvalues are complex. Let $ f $ be such a quantity. Then we can introduce its complex conjugate quantity $ f^* $, whose eigenvalues are the complex conjugates of those of $ f $. We denote by$ \hat{f}^\dag $ the operator corresponding to the quantity $ f^* $. It is called the \textit{Hermitian conjugate} of the operator $ \hat{f} $ and, in general, will be different from the definition of the operator $ \hat{f}^* $: the mean value of the quantity $ f^* $ in a state $ \Psi $ is
\[ \overline{f^*}=\int\Psi^*\hat{f}^\dag\Psi\mathrm{d}q. \]
We also have
\begin{align*}
\left(\overline{f}\right)^*&=\left[ \int\Psi^*\hat{f}\Psi\mathrm{d}q\right]^*\\
&=\int\Psi\hat{f}^*\Psi^*\mathrm{d}q\\
&=\int\Psi^*\tilde{\hat{f}}^*\Psi\mathrm{d}q.
\end{align*}
Equating these two expressions gives
\begin{equation}\label{3.16}
\hat{f}^\dag=\tilde{\hat{f}}^*,
\end{equation}
from which it is clear that $ \hat{f}^\dag $ is in general not the same as $ \hat{f}^* $.

The condition \eqref{3.15} can now be written
\begin{equation}\label{3.17}
\hat{f}=\hat{f}^\dag,
\end{equation}
i.e. the operator of a real physical quantity is the same as its Hermitian conjugate (Hermitian operators are also called \textit{self-conjugate}).

We shall show how the orthogonality of the eigenfunctions of an Hermitian operator corresponding to different eigenvalues can be directly proved. Let $ f_n $ and $ f_m $ be two different eigenvalues of the real quantity $ f $, and $ \Psi_n $, $ \Psi_m $ the corresponding eigenfunctions:
\[ \hat{f}\Psi_n=f_n\Psi_n,\hat{f}\Psi_m=f_m\Psi_m. \]


Multiplying both sides of the first of these equations by $ \Psi_m^* $, and both sides of the complex conjugate of the second by $ \Psi_n $, and subtracting corresponding terms, we find
\[ \Psi_m^*\hat{f}\Psi_n-\Psi_n\hat{f}^*\Psi_m^*=(f_n-f_m)\Psi_n\Psi_m^*. \]
We integrate both sides of this equation over $ q $. Since $ \hat{f}^* = \tilde{\hat{f}} $, by \eqref{3.14} the integral on the left-hand side of the equation is zero, so that we have
\[ (f_n-f_m)\int\Psi_n\Psi_m^*\mathrm{d}q=0 \]
whence, since $ f_n \ne f_m $, we obtain the required orthogonality property of the functions $ \Psi_n $ and $ \Psi_m $.

We have spoken here of only one physical quantity $ f $, whereas, as we said at the beginning of this section, we should have spoken of a complete set of simultaneously measurable physical quantities. We should then have found that to each of these quantities $ f,g,\dots $ there corresponds its operator $ \hat{f},\hat{g},\dots $. The eigenfunctions $ \Psi_n $ then correspond to states in which all the quantities concerned have definite values, i.e. they correspond to definite sets of eigenvalues $ f_n, g_n,\dots, $ and are simultaneous solutions of the system of equations
\[ \hat{f}\Psi=f\Psi,\quad\hat{g}\Psi=g\Psi,\dots. \]



\section{Addition and multiplication of operators}\label{Addition and multiplication of operators}
If $ \hat{f} $ and $ \hat{g} $ are the operators corresponding to two physical quantities $ f $ and $ g $, the sum $ f+g $ has a corresponding operator $ \hat{f}+\hat{g} $. However, the significance of adding different physical quantities in quantum mechanics depends considerably on whether the quantities are or are not simultaneously measurable. If $ f $ and $ g $ are simultaneously measurable, the operators $ \hat{f} $ and $ \hat{g} $ have common eigenfunctions, which are also eigenfunctions of the operator $ \hat{f}+\hat{g} $, and the eigenvalues of the latter operator are equal to the sums $ f_n + g_n $. But if $ f $ and $ g $ cannot simultaneously take definite values, their sum $ f+g $ has a more restricted significance. We can assert only that the mean value of this quantity in any state is equal to the sum of the mean values of the separate quantities:
\begin{equation}\label{4.1}
\overline{f+g}=\bar{f}+\bar{g}.
\end{equation}
The eigenvalues and eigenfunctions of the operator $ \hat{f}+\hat{g} $ will not, in general, now bear any relation to those of the quantities $ f $ and $ g $. It is evident that, if the operators $ \hat{f} $ and $ \hat{g} $ are Hermitian, the operator $ \hat{f}+\hat{g} $ will be so too, so that its eigenvalues are real and are equal to those of the new quantity $ f + g $ thus defined.

The following theorem should be noted. Let $ f_0 $ and $ g_0 $ be the smallest eigenvalues of the quantities $ f $ and $ g $, and $ (f+g)_0 $ that of the quantity $ f+g $. Then
\begin{equation}\label{4.2}
(f+g)_0\geqslant f_0+g_0
\end{equation}
The equality holds if $ f $ and $ g $ can be measured simultaneously. The proof follows from the obvious fact that the mean value of a quantity is always greater than or equal to its least eigenvalue. In a state in which the quantity $ f+g $ has the value $ (f+g)_0 $ we have $ \overline{f+g}=(f+g)_0 $ , and since, on the other hand, $ \overline{f+g}=\bar{f}+\bar{g}\geqslant f_0+g_0 $, we arrive at the inequality \eqref{4.2}.

Next, let $ f $ and $ g $ once more be quantities that can be measured simultaneously. Besides their sum, we can also introduce the concept of their \textit{product} as being a quantity whose eigenvalues are equal to the products of those of the quantities $ f $ and $ g $. It is easy to see that, to this quantity, there corresponds an operator whose effect consists of the successive action on the function of first one and then the other operator. Such an operator is represented mathematically by the product of the operators $ \hat{f} $ and $ \hat{g} $. For, if $ \Psi_n $ are the eigenfunctions common to the operators $ \hat{f} $ and $ \hat{g} $, we have
\[ \hat{f}\hat{g}\Psi_n=\hat{f}(\hat{g}\Psi_n)=\hat{f}g_n\Psi_n=g_n\hat{f}\Psi_n=g_nf_n\Psi_n \]
(the symbol $ \hat{f}\hat{g} $ denotes an operator whose effect on a function $ \Psi $ consists of the successive action first of the operator $ \hat{g} $ on the function $ \Psi $ and then of the operator $ \hat{f} $ on the function $ \hat{g}\Psi $). We could equally well take the operator $ \hat{g}\hat{f} $ instead of $\hat{f}\hat{g} $, the former differing from the latter in the order of its factors. It is obvious that the result of the action of either of these operators on the functions $ \Psi_n $ will be the same. Since, however, every wave function $ \Psi $ can be represented as a linear combination of the functions $ \Psi_n $, it follows that the result of the action of the operators $ \hat{f}\hat{g} $ and $ \hat{g}\hat{f} $ on an arbitrary function will also be the same. This fact can be written in the form of the symbolic equation $ \hat{f}\hat{g}=\hat{g}\hat{f} $ or
\begin{equation}\label{4.3}
\hat{f}\hat{g}-\hat{g}\hat{f}=0
\end{equation}

Two such operators $ \hat{f} $ and $ \hat{g} $ are said to be \textit{commutative}, or to \textit{commute} with each other. Thus we arrive at the important result: if two quantities $ f $ and $ g $ can simultaneously take definite values, then their operators commute with each other.

The converse theorem can also be proved (§11): if the operators $ \hat{f} $ and $ \hat{g} $ commute, then all their eigenfunctions can be taken common to both; physically, this means that the corresponding physical quantities can be measured simultaneously. Thus the commutability of the operators is a necessary and sufficient condition for the physical quantities to be simultaneously measurable.

A particular case of the product of operators is an operator raised to some power. From the above discussion we can deduce that the eigenvalues of an operator $ \hat{f}^p $ (where $ p $ is an integer) are equal to the $ p $th powers of the eigenvalues of the operator $ \hat{f} $. Any function $ \phi(\hat{f}) $ of an operator can be defined as an operator whose eigenvalues are equal to the same function $ \phi(f) $ of the eigenvalues of the operator $ \hat{f} $. If the function $ \phi(f) $ can be expanded as a Taylor series, this expresses the effect of the operator $ \phi(\hat{f}) $ in terms of those of various powers $ \hat{f}^p $.

In particular, the operator $ \hat{f}^{-1} $ is called the inverse of the operator $ \hat{f} $. It is evident that the successive action of the operators $ \hat{f} $ and $ \hat{f}^{-1} $ on any function leaves the latter unchanged, i.e. $\hat{f}\hat{f}^{-1}=\hat{f}^{-1}\hat{f}=1  $.

If the quantities $ f $ and $ g $ cannot be measured simultaneously, the concept of their product does not have the same direct meaning. This appears in the fact that the operator $ \hat{f}\hat{g} $ is not Hermitian in this case, and hence cannot correspond to any real physical quantity. For, by the definition of the transpose of an operator we can write
\[ \int\Psi\hat{f}\hat{g}\Phi\mathrm{d}q=\int\Psi\hat{f}(\hat{g}\Phi)\mathrm{d}q=\int(\hat{g}\Phi)(\tilde{\hat{f}}\Psi)\mathrm{d}q. \]
Here the operator $ \tilde{\hat{f}} $ acts only on the function $ \Psi $, and the operator $ \hat{g} $ on $ \Phi $, so that the integrand is a simple product of two functions $ \hat{g}\Phi $ and $ \tilde{\hat{f}}\Psi $. Again using the definition of the transpose of an operator, we can write
\[ \int\Psi\hat{f}\hat{g}\Phi\mathrm{d}q=\int(\tilde{\hat{f}}\Psi)(\hat{g}\Phi)\mathrm{d}q=\int\Phi\tilde{\hat{g}}\tilde{\hat{f}}\Phi\mathrm{d}q. \]

Thus we obtain an integral in which the functions $ \Psi $ and $ \Phi $ have changed places as compared with the original one. In other words, the operator $ \tilde{\hat{g}}\tilde{\hat{f}} $ is the transpose of $ \hat{f}\hat{g} $, and we can write
\begin{equation}\label{4.4}
\widetilde{\hat{f}\hat{g}}=\tilde{\hat{g}}\tilde{\hat{f}}
\end{equation}
i.e. the transpose of the product $ \hat{f}\hat{g} $ is the product of the transposes of the factors written in the opposite order. Taking the complex conjugate of both sides of equation \eqref{4.4}, we have
\begin{equation}\label{4.5}
(\hat{f}\hat{g})^\dag=\hat{g}^\dag\hat{f}^\dag
\end{equation}

If each of the operators $ \hat{f} $ and $ \hat{g} $ is Hermitian, then $(\hat{f}\hat{g})^\dag=\hat{g}\hat{f}  $. It follows from this that the operator $ \hat{f}\hat{g} $ is Hermitian if and only if the factors $ \hat{f} $ and $ \hat{g} $ commute.

We note that, from the products $ \hat{f}\hat{g} $ and $ \hat{g}\hat{f} $ of two non-commuting Hermitian operators, we can form an Hermitian operator, the \textit{symmetrized product}
\begin{equation}\label{4.6}
\frac{1}{2}(\hat{f}\hat{g}+\hat{g}\hat{f})
\end{equation}


It is easy to see that the difference $ \hat{f}\hat{g}-\hat{g}\hat{f} $ is an \textit{anti-Hermitian} operator (i.e. one for which the transpose is equal to the complex conjugate taken with the opposite sign). It can be made Hermitian by multiplying by $ \mathrm{i} $; thus
\begin{equation}\label{4.7}
\mathrm{i}(\hat{f}\hat{g}-\hat{g}\hat{f})
\end{equation}
is again an Hermitian operator.

In what follows we shall sometimes use for brevity the notation
\begin{equation}\label{4.8}
\{ \hat{f},\hat{g} \}=\hat{f}\hat{g}-\hat{g}\hat{f}
\end{equation}
called the commutator of these operators. It is easily seen that
\begin{equation}\label{4.9}
\{\hat{f}\hat{g},\hat{h}\}=\{\hat{f},\hat{h}\}\hat{g}+\hat{f}{\hat{g},\hat{h}}
\end{equation}
We notice that, if $ \{\hat{f},\hat{h}\} = 0 $ and $ \{\hat{g},\hat{h}\}=0 $, it does not in general follow that $ \hat{f} $ and $ \hat{g} $ commute.
\section{The continuous spectrum}\label{The continuous spectrum}
All the relations given in §\ref{Operators} and \ref{Addition and multiplication of operators}, describing the properties of the eigenfunctions of a discrete spectrum, can be generalized without difficulty to the case of a continuous spectrum of eigenvalues.
	
Let $ f $ be a physical quantity having a continuous spectrum. We shall denote its eigenvalues by the same letter $ f $ simply, and the corresponding eigenfunctions by $ \Psi_f $. Just as an arbitrary wave function $ \Psi $ can be expanded in a series \eqref{3.2} of eigenfunctions of a quantity having a discrete spectrum, it can also be expanded (this time as an integral) in terms of the complete set of eigenfunctions of a quantity with a continuous spectrum. This expansion has the form
\begin{equation}\label{5.1}
\Psi(q)=\int a_f\Psi_f(q)\mathrm{d}f
\end{equation}
where the integration is extended over the whole range of values that can be taken by the quantity $ f $.
	
The subject of the normalization of the eigenfunctions of a continuous spectrum is more complex than in the case of a discrete spectrum. The requirement that the integral of the squared modulus of the function should be equal to unity cannot here be satisfied, as we shall see below. Instead, we try to normalize the functions $ \Psi_f $ in such a way that $ |a_f|^2\mathrm{d}f $ is the probability that the physical quantity concerned, in the state described by the wave function $ \Psi $, has a value between $ f $ and $ f + \mathrm{d}f $. Since the sum of the probabilities of all possible values of $ f $ must be equal to unity, we have
\begin{equation}\label{5.2}
\int|a_f|^2\mathrm{d}f=1
\end{equation}
(similarly to the relation \eqref{3.3} for a discrete spectrum).
	
Proceeding in exactly the same way as in the derivation of formula \eqref{3.5}, and using the same arguments, we can write, firstly,
\[ \int\Psi\Psi^*\mathrm{d}q=\int|a_f|^2\mathrm{d}f \]
and, secondly,
\[ \int\Psi\Psi^*\mathrm{d}q=\iint a_f^*\Psi_f^*\Psi\mathrm{d}f\mathrm{d}q. \]	
	
By comparing these two expressions we find the formula which determines the expansion coefficients,
\begin{equation}\label{5.3}
a_f=\int\Psi(q)\Psi_f^*(q)\mathrm{d}q,
\end{equation}	
in exact analogy to \eqref{3.5}.
	
To derive the normalization condition, we now substitute \eqref{5.1} in \eqref{5.3}, and obtain
\[ a_f=\int a_{f'}\left(\int \Psi_{f'}\Psi_{f}^*\mathrm{d}q\right)\mathrm{d}f' \]	
	
This relation must hold for arbitrary $ a_f $, and therefore must be satisfied identically. For this to be so, it is necessary that, first of all, the coefficient of $ a_f $, in the integrand (i.e. the integral $ \int \Psi_{f'}\Psi_{f}^*\mathrm{d}q $) should be zero for all $ f′ \ne f $. For $ f′ = f $, this coefficient must become infinite (otherwise the integral over $ f′ $ would vanish). Thus the integral $ \int \Psi_{f'}\Psi_{f}^*\mathrm{d}q $ is a function of the difference $ f'-f $, which becomes zero for values of the argument different from zero and is infinite when the argument is zero. We denote this function by $ \delta(f'-f) $:
\begin{equation}\label{5.4}
\int \Psi_{f'}\Psi_{f}^*\mathrm{d}q=\delta(f'-f)
\end{equation}	

The manner in which the function $ \delta(f'-f) $ becomes infinite for $ f'-f = 0 $ is determined by the fact that we must have
\[ \int \delta(f'-f)a_{f'}\mathrm{d}f'=a_f \]
It is clear that, for this to be so, we must have
\[ \int\delta(f'-f)\mathrm{d}f'=1 \]	
	
The function thus defined is called a \textit{delta function}, and was first used in theoretical physics by P. A. M. Dirac. We shall write out once more the formulae which define it. They are
\begin{equation}\label{5.5}
\delta(x)=0\text{ for }x\ne0,\qquad\delta(0)=\infty,
\end{equation}
while
\begin{equation}\label{5.6}
\int_{-\infty}^{+\infty}\delta(x)\mathrm{d}x=1.
\end{equation}	
We can take as limits of integration any numbers such that $ x = 0 $ lies between them. If $ f (x) $ is some function continuous at $ x = 0 $, then
\begin{equation}\label{5.7}
\int_{-\infty}^{+\infty} \delta(x)f(x)\mathrm{d}x=f(0)
\end{equation}	
This formula can be written in the more general form
\begin{equation}\label{5.8}
\int\delta(x-a)f(x)\mathrm{d}x=f(a)
\end{equation}	
where the range of integration includes the point $ x = a $, and $ f (x) $ is continuous at $ x = a $. It is also evident that
\begin{equation}\label{5.9}
\delta(-x)=\delta(x)
\end{equation}	
i.e. the delta function is even. Finally, writing
\[ \int_{-\infty}^{+\infty}\delta(\alpha x)\mathrm{d}x=\int_{-\infty}^{+\infty}\delta(y)\frac{\mathrm{d}y}{|\alpha|}=\frac{1}{|\alpha|}\]	
we can deduce that
\begin{equation}\label{5.10}
\delta(\alpha x)= \frac{1}{|\alpha|}\delta(x)
\end{equation}	
where $ \alpha $ is any constant.
	
The formula \eqref{5.4} gives the normalization rule for the eigenfunctions of a continuous spectrum; it replaces the condition \eqref{3.6} for a discrete spectrum. We see that the functions $ \Psi_f $ and $ \Psi_{f'} $ with $ f\ne f' $ are, as before, orthogonal. However, the integrals of the squared moduli $ |\Psi_{f}|^2 $ of the functions diverge for a continuous spectrum.
	
The functions $ \Psi_{f}(q) $ satisfy still another relation similar to \eqref{5.4}. To derive this, we substitute \eqref{5.3} in \eqref{5.1}, which gives
\[ \Psi(q)=\int\Psi(q')\left( \int\Psi_{f}^*(q')\Psi_{f}(q)\mathrm{d}f\right)\mathrm{d}q' \]	
whence we can at once deduce that we must have
\begin{equation}\label{5.11}
\int\Psi_{f}^*(q')\Psi_{f}(q)\mathrm{d}f=\delta(q'-q)
\end{equation}	
	
There is, of course, an analogous relation for a discrete spectrum:
\begin{equation}\label{5.12}
\sum_{n}\Psi_n^*(q')\Psi_n(q)=\delta(q'-q)
\end{equation}	

Comparing the pair of formulae \eqref{5.1}, \eqref{5.4} with the pair \eqref{5.3}, \eqref{5.11}, we see that, on the one hand, the function $ \Psi(q) $ can be expanded in terms of the functions$  \Psi_f(q) $ with expansion coefficients $ a_f $ and, on the other hand, formula \eqref{5.3} represents an entirely analogous expansion of the function $ a_f \equiv a (f) $ in terms of the functions $  \Psi_f^*(q) $, while the $ \Psi(q) $ play the part of expansion coefficients. The function $ a (f) $, like $ \Psi(q) $, completely determines the state of the system; it is sometimes called a wave function \textit{in the $ f $ representation} (while the function $ \Psi(q) $ is called a wave function in the \textit{$ q $ representation}). Just as $ |\Psi(q)|^2 $ determines the probability for the system to have coordinates lying in a given interval $ \mathrm{d}q $, so $ |a(f)|^2 $ determines the probability for the values of the quantity $ f $ to lie in a given interval $ \mathrm{d}f $. On the one hand, the functions $ \Psi_f(q) $ are the eigenfunctions of the quantity $ f $ in the $ q $ representation; on the other hand, their complex conjugates are the eigenfunctions of the coordinate $ q $ in the $ f $ representation.
	
Let $ \phi(f) $ be some function of the quantity $ f $, such that $ \phi $ and $ f $ are related in a one-to-one manner. Each of the functions $ \Psi_f(q) $ can then be regarded as an eigenfunction of the quantity $ \phi $. Here, however, the normalization of these functions must be changed: the eigenfunctions $ \Psi_\phi(q) $ of the quantity φ must be normalized by the condition
\[ \int\Psi_{\phi(f')}\Psi_{\phi(f)}^*\mathrm{d}q=\delta\left[ \phi(f')-\phi(f)\right],\]	
whereas the functions $ \Psi_f $, are normalized by the condition \eqref{5.4}. The argument of the delta function becomes zero only for $ f' = f $. As $ f' $ approaches $ f $, we have 
\[ \phi(f')-\phi(f) =\frac{\mathrm{d}\phi(f)}{\mathrm{d}f}(f'-f). \]
By \eqref{5.10} we can therefore write\footnote{In general, if $ \phi(x) $ is some one-valued function (the inverse function need not be one-valued), we have
\begin{equation*}\label{5.13a}
\delta\left[\phi(x)\right]=\sum_{i}\frac{1}{|\phi'(\alpha_i)|}\delta(x-\alpha_i)\tag{5.13a}
\end{equation*}	
where $ \alpha_i $ are the roots of the equation $ \phi(x) = 0 $.
}
\begin{equation}\label{5.13}
\delta\left[\phi(f')-\phi(f) \right]=\frac{1}{|\mathrm{d}\phi(f)/\mathrm{d}f|}\delta(f'-f)
\end{equation}	
Comparing this with \eqref{5.4}, we see that the functions $ \Psi_\phi $ and $ \Psi_f $ are related by
\begin{equation}\label{5.14}
\Psi_{\phi(f)}=\frac{1}{\sqrt{|\mathrm{d}\phi(f)/\mathrm{d}f|}}\Psi_f
\end{equation}	

There are also physical quantities which in one range of values have a discrete spectrum, and in another a continuous spectrum. For the eigenfunctions of such a quantity all the relations derived in this and the previous sections are, of course, true. It need only be noted that the complete set of functions is formed by combining the eigenfunctions of both spectra. Hence the expansion of an arbitrary wave function in terms of the eigenfunctions of such a quantity has the form
\begin{equation}\label{5.15}
\Psi(q)=\sum_{n}a_n\Psi_n(q)+\int a_f\Psi_f(q)\mathrm{d}f
\end{equation}	
where the sum is taken over the discrete spectrum and the integral over the whole continuous spectrum.
	
The coordinate $ q $ itself is an example of a quantity having a continuous spectrum. It is easy to see that the operator corresponding to it is simply multiplication by $ q $. For, since the probability of the various values of the coordinate is determined by the square $ |\Psi(q)|^2 $, the mean value of the coordinate is
\[ \bar{q}=\int q|\Psi|^2\mathrm{d}q=\int\Psi^*q\Psi\mathrm{d}q. \]	
Comparison of this with the definition \eqref{3.8} of an operator shows that\footnote{In future we shall always, for simplicity, write operators which amount to multiplication by some quantity in the form of that quantity itself.
}
\begin{equation}\label{5.16}
\hat{q}=q.
\end{equation}	
The eigenfunctions of this operator must be determined, according to the usual rule, by the equation $ q\Psi_{q_0} = q_0\Psi{q_0} $, where $ q_0 $ temporarily denotes the actual values of the coordinate as distinct from the variable $ q $. Since this equation can be satisfied either by $ \Psi_{q_0} = 0 $ or by $ q = q_0 $, it is clear that the eigenfunctions which satisfy the normalization condition are\footnote{The expansion coefficients for an arbitrary function $ \Psi $ in terms of these eigenfunctions are
\[ a_{q_0}=\int\Psi(q)\delta(q-q_0)\mathrm{d}q=\Psi(q_0). \]	
The probability that the value of the coordinate lies in a given interval $ \mathrm{d}q_0 $ is
\[ |a_{q_0}|^2\mathrm{d}q_0=|\Psi(q_0)|^2\mathrm{d}q_0, \]	
as it should be.
}
\begin{equation}\label{5.17}
\Psi_{q_0}=\delta(q-q_0)
\end{equation}
\section{The passage to the limiting case of classical mechanics}\label{The passage to the limiting case of classical mechanics}	
Quantum mechanics contains classical mechanics in the form of a certain limiting case. The question arises as to how this passage to the limit is made.
	
In quantum mechanics an electron is described by a wave function which determines the various values of its coordinates; of this function we so far know only that it is the solution of a certain linear partial differential equation. In classical mechanics, on the other hand, an electron is regarded as a material particle, moving in a path which is completely determined by the equations of motion. There is an interrelation, somewhat similar to that between quantum and classical mechanics, in electrodynamics between wave optics and geometrical optics. In wave optics, the electromagnetic waves are described by the electric and magnetic field vectors, which satisfy a definite system of linear differential equations, namely Maxwell’s equations. In geometrical optics, however, the propagation of light along definite paths, or rays, is considered. Such an analogy enables us to see that the passage from quantum mechanics to the limit of classical mechanics occurs similarly to the passage from wave optics to geometrical optics.
	
Let us recall how this latter transition is made mathematically (see Fields, §53). Let $ u $ be any of the field components in the electromagnetic wave. It can be written in the form $ u = a\mathrm{e}^{\mathrm{i}\phi} $ (with $ a $ and $ \phi $ real), where $ a $ is called the amplitude and $ \phi $ the phase of the wave (called in geometrical optics the eikonal). The limiting case of geometrical optics corresponds to small wavelengths; this is expressed mathematically by saying that $ \phi $ varies by a large amount over short distances; this means, in particular, that it can be supposed large in absolute value.
	
Similarly, we start from the hypothesis that, to the limiting case of classical mechanics, there correspond in quantum mechanics wave functions of the form$  \Psi = a\mathrm{e}^{\mathrm{i}\phi} $, where $ a $ is a slowly varying function and $ \phi $ takes large values. As is well known, the path of a particle can be determined in mechanics by means of the variational principle, according to which what is called the action $ S $ of a mechanical system must take its least possible value (the principle of least action). In geometrical optics the path of the rays is determined by what is called Fermat’s principle, according to which the optical path length of the ray, i.e. the difference between its phases at the beginning and end of the path, must take its least (or greatest) possible value.
	
On the basis of this analogy, we can assert that the phase $ \phi $ of the wave function, in the limiting (classical) case, must be proportional to the mechanical action $ S $ of the physical system considered, i.e. we must have $ S = \mathrm{const}\phi $. The constant of proportionality is called Planck’s constant\footnote{It was introduced into physics by M. Planck in 1900. The constant $ \hbar $ which we use everywhere in this book, is, strictly speaking, Planck’s constant divided by $ 2\uppi $; this is Dirac’s notation.
} and is denoted by $ \hbar $. It has the dimensions of action (since $ \phi $ is dimensionless) and has the value
\[ \hbar=1.054\times10^{-27}\mathrm{erg}\cdot\mathrm{s} \]	
	
Thus, the wave function of an “almost classical” (or, as we say, \textit{quasi-classical}) physical system has the form
\begin{equation}\label{6.1}
\Psi=a\mathrm{e}^{\mathrm{i}S/\hbar}
\end{equation}	

Planck’s constant $\hbar$ plays a fundamental part in all quantum phenomena. Its relative value (compared with other quantities of the same dimensions) determines the “extent of quantization” of a given physical system. The transition from quantum mechanics to classical mechanics, corresponding to large phase, can be formally described as a passage to the limit $ \hbar \rightarrow 0 $ (just as the transition from wave optics to geometrical optics corresponds to a passage to the limit of zero wavelength, $ \lambda \rightarrow 0 $).
	
We have ascertained the limiting form of the wave function, but the question still remains how it is related to classical motion in a path. In general, the motion described by the wave function does not tend to motion in a definite path. Its connection with classical motion is that, if at some initial instant the wave function, and with it the probability distribution of the coordinates, is given, then at subsequent instants this distribution will change according to the laws of classical mechanics (for a more detailed discussion of this, see the end of §17).
	
In order to obtain motion in a definite path, we must start from a wave function of a particular form, which is perceptibly different from zero only in a very small region of space (what is called a wave packet); the dimensions of this region must tend to zero with $\hbar$. Then we can say that, in the quasi-classical case, the wave packet will move in space along a classical path of a particle.
	
Finally, quantum-mechanical operators must reduce, in the limit, simply to multiplication by the corresponding physical quantity.
\section{The wave function and measurements}\label{The wave function and measurements}	
Let us again return to the process of measurement, whose properties have been qualitatively discussed in §\ref{The uncertainty principle}; we shall show how these properties are related to the mathematical formalism of quantum mechanics.
	
We consider a system consisting of two parts: a classical apparatus and an electron (regarded as a quantum object). The process of measurement consists in these two parts’ coming into interaction with each other, as a result of which the apparatus passes from its initial state into some other; from this change of state we draw conclusions concerning the state of the electron. The states of the apparatus are distinguished by the values of some physical quantity (or quantities) characterizing it—the “readings of the apparatus”. We conventionally denote this quantity by $ g $, and its eigenvalues by $ g_n $; these take in general, in accordance with the classical nature of the apparatus, a continuous range of values, but we shall—merely in order to simplify the subsequent formulae—suppose the spectrum discrete. The states of the apparatus are described by means of quasi-classical wave functions, which we shall denote by $\Psi_n(\xi)$, where the suffix $ n $ corresponds to the “reading” $ g_n $ of the apparatus, and $\xi$ denotes the set of its coordinates. The classical nature of the apparatus appears in the fact that, at any given instant, we can say with certainty that it is in one of the known states $\Psi_n$ with some definite value of the quantity $ g $; for a quantum system such an assertion would, of course, be unjustified.
	
Let $\Phi_0(\xi)$ be the wave function of the initial state of the apparatus (before the measurement), and $ \Psi(q) $ some arbitrary normalized initial wave function of the electron ($ q $ denoting its coordinates). These functions describe the state of the apparatus and of the electron independently, and therefore the initial wave function of the whole system is the product
\begin{equation}\label{7.1}
\Psi(q)\Phi_0(\xi)
\end{equation}	
Next, the apparatus and the electron interact with each other. Applying the equations of quantum mechanics, we can in principle follow the change of the wave function of the system with time. After the measuring process it may not, of course, be a product of functions of $\xi$ and $ q $. Expanding the wave function in terms of the eigenfunctions $\Phi_n$ of the apparatus (which form a complete set of functions), we obtain a sum of the form
\begin{equation}\label{7.2}
\sum_{n}A_n(q)\Psi_n(\xi)
\end{equation}	
where the $ A_n(q) $ are some functions of q.
	
The classical nature of the apparatus, and the double role of classical mechanics as both the limiting case and the foundation of quantum mechanics, now make their appearance. As has been said above, the classical nature of the apparatus means that, at any instant, the quantity $ g $ (the “reading of the apparatus“) has some definite value. This enables us to say that the state of the system apparatus $ + $ electron after the measurement will in actual fact be described, not by the entire sum (7.2), but by only the one term which corresponds to the “reading” gn of the apparatus,
\begin{equation}\label{7.3}
A_n(q)\Psi_n(\xi)
\end{equation}	
It follows from this that $ A_n(q) $ is proportional to the wave function of the electron after the measurement. It is not the wave function itself, as is seen from the fact that the function $ A_n(q) $ is not normalized. It contains both information concerning the properties of the resulting state of the electron and the probability (determined by the initial state of the system) of the occurrence of the $ n $th “reading” of the apparatus.
	
Since the equations of quantum mechanics are linear, the relation between $ A_n(q) $ and the initial wave function of the electron $ \Psi(q) $ is in general given by some linear integral operator:
\begin{equation}\label{7.4}
A_n(q)=\int K_n(q,q')\Psi(q')\mathrm{d}q'
\end{equation}	
with a kernel $ K_n(q, q') $ which characterizes the measurement process concerned.
		
We shall suppose that the measurement concerned is such that it gives a complete description of the state of the electron. In other words (see §\ref{The uncertainty principle}), in the resulting state the probabilities of all the quantities must be independent of the previous state of the electron (before the measurement). Mathematically, this means that the form of the functions $ A_n(q) $ must be determined by the measuring process itself, and does not depend on the initial wave function $ \Psi(q) $ of the electron. Thus the $ A_n $ must have the form
\begin{equation}\label{7.5}
A_n(q)=a_n\phi_n(q)
\end{equation}		
where the $\phi_n$ are definite functions, which we suppose normalized, and only the constants $ a_n $ depend on $ \Psi(q) $. In the integral relation \eqref{7.4} this corresponds to a kernel $ K_n(q, q') $ which is a product of a function of $ q $ and a function of $ q' $:
\begin{equation}\label{7.6}
K_n(q,q')=\phi_n(q)\Psi_n^*(q')
\end{equation}		
then the linear relation between the constants an and the function $ \Psi(q) $ is
\begin{equation}\label{7.7}
a_n=\int\Psi(q)\Psi_n^*(q)\mathrm{d}q
\end{equation}		
where the $ \Psi_n(q) $ are certain functions depending on the process of measurement.
		
The functions $ \phi_n(q) $ are the normalized wave functions of the electron after measurement. Thus we see how the mathematical formalism of the theory reflects the possibility of finding by measurement a state of the electron described by a definite wave function.
		
If the measurement is made on an electron with a given wave function $ \Psi(q) $, the constants $ a_n $ have a simple physical meaning: in accordance with the usual rules, $ |a_n|^2 $ is the probability that the measurement will give the $ n $th result. The sum of the probabilities of all results is equal to unity:
\begin{equation}\label{7.8}
\sum_{n}|a_n|^2=1
\end{equation}		

In order that equations \eqref{7.7} and \eqref{7.8} should hold for an arbitrary normalized function $ \Psi(q) $, it is necessary (cf. §\ref{Operators}) that an arbitrary function $ \Psi(q) $ can be expanded in terms of the functions $ \Psi_n(q) $. This means that the functions $ \Psi_n(q) $ form a complete set of normalized and orthogonal functions.
		
If the initial wave function of the electron coincides with one of the functions $ \Psi_n(q) $, then the corresponding constant $ a_n $ is evidently equal to unity, while all the others are zero. In other words, a measurement made on an electron in the state $ \Psi_n(q) $ gives with certainty the $ n $th result.
		
All these properties of the functions $ \Psi_n(q) $ show that they are the eigenfunctions of some physical quantity (denoted by $ f $) which characterizes the electron, and the measurement concerned can be spoken of as a measurement of this quantity.
		
It is very important to notice that the functions $ \Psi_n(q) $ do not, in general, coincide with the functions $ \phi_n(q) $; the latter are in general not even mutually orthogonal, and do not form a set of eigenfunctions of any operator. This expresses the fact that the results of measurements in quantum mechanics cannot be reproduced. If the electron was in a state $ \Psi_n(q) $, then a measurement of the quantity $ f $ carried out on it leads with certainty to the value $ f_n $. After the measurement, however, the electron is in a state $ \phi_n(q) $ different from its initial one, and in this state the quantity $ f $ does not in general take any definite value. Hence, on carrying out a second measurement on the electron immediately after the first, we should obtain for $ f $ a value which did not agree with that obtained from the first measurement.\footnote{There is, however, an important exception to the statement that results of measurements cannot be reproduced: the one quantity the result of whose measurement can be exactly reproduced is the coordinate. Two measurements of the coordinates of an electron, made at a sufficiently small interval of time, must give neighbouring values; if this were not so, it would mean that the electron had an infinite velocity. Mathematically, this is related to the fact that the coordinate commutes with the operator of the interaction energy between the electron and the apparatus, since this energy is (in non-relativistic theory) a function of the coordinates only.
} To predict (in the sense of calculating probabilities) the result of the second measurement from the known result of the first, we must take from the first measurement the wave function $ \phi_n(q) $ of the state in which it resulted, and from the second measurement the wave function $ \Psi_n(q) $ of the state whose probability is required. This means that from the equations of quantum mechanics we determine the wave function $ \phi_n(q,t) $ which, at the instant when the first measurement is made, is equal to $ \phi_n(q) $; the probability of the $ m $th result of the second measurement, made at time t, is then given by the squared modulus of the integral $ \int \phi_n(q, t)\Psi_m^* (q) dq $.
		
We see that the measuring process in quantum mechanics has a “two-faced” character: it plays different parts with respect to the past and future of the electron. With respect to the past, it “verifies” the probabilities of the various possible results predicted from the state brought about by the previous measurement. With respect to the future, it brings about a new state (see also §44). Thus the very nature of the process of measurement involves a far-reaching principle of irreversibility.
		
This irreversibility is of fundamental significance. We shall see later (at the end of §18) that the basic equations of quantum mechanics are in themselves symmetrical with respect to a change in the sign of the time; here quantum mechanics does not differ from classical mechanics. The irreversibility of the process of measurement, however, causes the two directions of time to be physically non-equivalent, i.e. creates a difference between the future and the past.
		
		
		
		



